@article{Hsia2016,
author = {Hsia, Chih-Hsien and Chiang, Jen-Shiun and Hsieh, Chi-Fang},
doi = {10.1007/s11042-015-2714-2},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Hsia, Chiang, Hsieh{\_}2016{\_}Low-complexity range tree for video synopsis system.pdf:pdf},
issn = {1380-7501},
journal = {Multimedia Tools and Applications},
keywords = {Gaussian mixture model,Low-complexity range tree,Object detection,Video retrieval system,Video synopsis system},
month = {aug},
number = {16},
pages = {9885--9902},
publisher = {Multimedia Tools and Applications},
title = {{Low-complexity range tree for video synopsis system}},
url = {http://dx.doi.org/10.1007/s11042-015-2714-2 http://link.springer.com/10.1007/s11042-015-2714-2},
volume = {75},
year = {2016}
}
@inproceedings{Jin2016,
author = {Jin, Jing and Liu, Feng and Gan, Zongliang and Cui, Ziguan},
booktitle = {2016 8th International Conference on Wireless Communications {\&} Signal Processing (WCSP)},
doi = {10.1109/WCSP.2016.7752708},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Jin et al.{\_}2016{\_}Online video synopsis method through simple tube projection strategy.pdf:pdf},
isbn = {978-1-5090-2860-3},
keywords = {buffer,collision,projection matrix,video synopsis},
month = {oct},
pages = {1--5},
publisher = {IEEE},
title = {{Online video synopsis method through simple tube projection strategy}},
url = {http://ieeexplore.ieee.org/document/7752708/},
volume = {1},
year = {2016}
}
@article{Mahapatra2016,
abstract = {In this paper, we present a framework for generating a synopsis of multi-view videos that are acquired from a surveillance site, indoor or outdoor, using multiple cameras. The synopsis generation is modeled as a scheduling problem that we solve using three separate approaches: table-driven approach, contradictory binary graph coloring (CBGC) approach, and simulated annealing (SA) based approach. An action recognition module is included in the framework to recognize important actions performed by various humans present in the videos. Inclusion of such important actions in the synopsis has helped to reduce its length significantly. The synopsis length is further reduced through a post-processing step that computes the visibility score for each object track using a fuzzy inference system. Among the three proposed schemes, maximum reduction in synopsis length is obtained through the CBGC approach. The stochastic approach using SA, on the other hand, achieves a better trade-off among the multiple optimization criteria. Experimental evaluations on standard datasets demonstrate the efficacy of the proposed framework over its counterparts concerning the reduction in synopsis length and retention of important actions.},
author = {Mahapatra, Ansuman and Sa, Pankaj K. and Majhi, Banshidhar and Padhy, Sudarshan},
doi = {10.1016/j.image.2016.01.002},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Mahapatra et al.{\_}2016{\_}MVS A multi-view video synopsis framework.pdf:pdf},
isbn = {9781479983391},
issn = {09235965},
journal = {Signal Processing: Image Communication},
keywords = {Multi-camera network,Multi-view video,Video summarization,Video synopsis},
month = {mar},
pages = {31--44},
publisher = {Elsevier},
title = {{MVS: A multi-view video synopsis framework}},
url = {http://dx.doi.org/10.1016/j.image.2016.01.002 http://linkinghub.elsevier.com/retrieve/pii/S0923596516000059},
volume = {42},
year = {2016}
}
@article{Lin2015,
abstract = {{\textcopyright} 2015 Springer-Verlag Berlin Heidelberg Video synopsis is one of the popular research topics in the field of digital video and has broad application prospects. Current research of it focuses on the methods of generating video synopsis or studying to utilize optimization algorithms such as fuzzy theory, minimum sparse reconstruction, and genetic algorithm to optimize its computing steps. This paper mainly studies the object-based video synopsis technology in distributed environment. We propose an effective video synopsis algorithm and a distributed processing model to accelerate the computing speed of video synopsis. The algorithm is proposed for studies of surveillance videos, which focuses on several key algorithmic steps, for instance, initialization of original video resources, background modeling, moving object detecting, and nonlinear rearrangement. These steps can be performed in parallel. In order to obtain good video synopsis effect and fast computing speed, some optimization methods are applied to these steps. With the aim of employing much more computing resources, we propose a distributed processing model, which splits the original video file into multiple segments and distributes them to different computing nodes to improve the computing performance by leveraging the multi-core and multi-thread capabilities of CPU. Experimental results show that the proposed distributed model can significantly improve the computing speed of video synopsis.},
author = {Lin, Longxin and Lin, Weiwei and Xiao, Weijun and Huang, Sibin},
doi = {10.1007/s00500-015-1823-1},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Lin et al.{\_}2015{\_}An optimized video synopsis algorithm and its distributed processing model.pdf:pdf},
issn = {1432-7643},
journal = {Soft Computing},
keywords = {Distributed processing,Fuzzy C means,Fuzzy set theory,Genetic algorithm,Multi-thread,Sparse reconstruction,Video surveillance,Video synopsis},
month = {feb},
number = {4},
pages = {935--947},
publisher = {Springer Berlin Heidelberg},
title = {{An optimized video synopsis algorithm and its distributed processing model}},
url = {http://link.springer.com/10.1007/s00500-015-1823-1},
volume = {21},
year = {2015}
}
@article{He2017,
annote = {이 논문의 논지 전개를 배울 필요가 있음.
Online Video Synopsis를 비교하는 방법으로 삼으면 좋을 것으로 예상됨.
그런데 결국 비교실험을 위한 구현을 해야될 것 같음... (공개된 데이터셋이 없음)
Potential Collision Graph 자체가 이 논문의 contribution임},
author = {He, Yi and Qu, Zhiguo and Gao, Changxin and Sang, Nong},
doi = {10.1109/LSP.2016.2633374},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/He et al.{\_}2017{\_}Fast Online Video Synopsis Based on Potential Collision Graph.pdf:pdf},
issn = {1070-9908},
journal = {IEEE Signal Processing Letters},
month = {jan},
number = {1},
pages = {22--26},
title = {{Fast Online Video Synopsis Based on Potential Collision Graph}},
url = {http://ieeexplore.ieee.org/document/7762044/},
volume = {24},
year = {2017}
}
@article{He2017a,
abstract = {Abstract Video synopsis is an intelligent condensation approach to solve fast video browsing and retrieval for surveillance cameras. However, collision caused by unsatisfied tube rearrangement in traditional methods brings uncomfortable visual effect to users and how to mitigate the collision still remains an attracting topic. Unlike conventional methods that deal with tube rearrangement by minimizing a global energy function, we propose a novel approach by formulating it as a graph coloring problem. In our approach, all the tubes are firstly mapped into the spatial domain for analyzing their potential collision relationship. The input tube set is then represented by a graph structure, where each node stands for a tube and the edge between two nodes represents the potential collision relationship. To mitigate the collision artifacts, our method finds the mapping of tubes from original video to synopsis video by L(q)-coloring the graph, which separates tubes from their collision points. The parameter q is left tunable to make a compromise between collision artifacts and synopsis length, which can better meet users' demand of freely adjusting the compactness of synopsis video. The shifted objects are finally composited with the background image to obtain the high-quality video synopsis. Extensive experimental results show that the proposed method can generate more compact video synopsis with less collision artifacts than the existing methods.},
author = {He, Yi and Gao, Changxin and Sang, Nong and Qu, Zhiguo and Han, Jun},
doi = {10.1016/j.neucom.2016.11.011},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/He et al.{\_}2017{\_}Graph coloring based surveillance video synopsis.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Graph coloring,Tube rearrangement,Video synopsis},
month = {feb},
number = {September 2016},
pages = {64--79},
title = {{Graph coloring based surveillance video synopsis}},
url = {http://www.sciencedirect.com/science/article/pii/S0925231216313406 http://linkinghub.elsevier.com/retrieve/pii/S0925231216313406},
volume = {225},
year = {2017}
}
@inproceedings{Hoshen2015,
abstract = {Video surveillance cameras generate most of recorded video, and there is far more recorded video than operators can watch. Much progress has recently been made using summarization of recorded video, but such techniques do not have much impact on live video surveillance. We assume a camera hierarchy where a Master camera observes the decision-critical region, and one or more Slave cameras observe regions where past activity is important for making the current decision. We propose that when people appear in the live Master camera, the Slave cameras will display their past activities, and the operator could use past information for real-time decision making. The basic units of our method are action tubes, representing objects and their trajectories over time. Our object-based method has advantages over frame based methods, as it can handle multiple people, multiple activities for each person, and can address re-identification uncertainty.},
archivePrefix = {arXiv},
arxivId = {1505.05254},
author = {Hoshen, Yedid and Peleg, Shmuel},
booktitle = {2015 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2015.7350790},
eprint = {1505.05254},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Hoshen, Peleg{\_}2015{\_}Live video synopsis for multiple cameras.pdf:pdf},
isbn = {978-1-4799-8339-1},
issn = {15224880},
keywords = {Multi Camera Synopsis,Video Surveillance,Video Synopsis},
month = {sep},
pages = {212--216},
publisher = {IEEE},
title = {{Live video synopsis for multiple cameras}},
url = {http://ieeexplore.ieee.org/document/7350790/},
volume = {2015-Decem},
year = {2015}
}
@article{RuiZhong2014,
abstract = {With the increasing volume of video data, how to analyze and browse video in a fast and effective way has become an urgent problem in applications. This letter proposes a novel video synopsis method in compressed domain for browsing video captured by static cameras. Synopsis video is a video abstraction, which displays moving objects from different periods simultaneously on the primary background contents of original video. To overcome the low efficiency of traditional video synopsis for compressed video, our method presents a new graph cut algorithm to extract objects tubes and meanwhile gives a fast solution to minimize energy function in compressed domain. Experimental results in H.264 video have demonstrated the high-efficiency of this new video synopsis scheme for massive video browsing.},
author = {{Rui Zhong} and {Ruimin Hu} and {Zhongyuan Wang} and {Shizheng Wang}},
doi = {10.1109/LSP.2014.2317754},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Rui Zhong et al.{\_}2014{\_}Fast Synopsis for Moving Objects Using Compressed Video.pdf:pdf},
issn = {1070-9908},
journal = {IEEE Signal Processing Letters},
keywords = {Algorithm design and analysis,Compressed domain,Computational complexity,Computational modeling,Electron tubes,H.264 video,Labeling,Streaming media,Three-dimensional displays,compressed video,data compression,energy function,graph cut algorithm,graph cuts and video browsing,massive video browsing,moving object display,objects tube extraction,static camera,video abstraction,video coding,video synopsis,video synopsis method},
month = {jul},
number = {7},
pages = {834--838},
title = {{Fast Synopsis for Moving Objects Using Compressed Video}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6805196 http://ieeexplore.ieee.org/document/6805196/},
volume = {21},
year = {2014}
}
@article{Li2016,
abstract = {With a growth of surveillance cameras, the amount of captured videos expands. Manually analyzing and retrieving surveillance video is labor intensive and expensive. It would be much more convenient to generate a video digest, with which we can view the video in a fast and motion-preserving way. In this paper, we propose a novel video synopsis approach to generate condensed video, which uses an object tracking method for extracting important objects. This method will generate video tubes and a seam carving method to condense the original video. Experimental results demonstrate that our proposed method can achieve a high condensation rate while preserving all the important objects of interest. Therefore, this approach can enable users to view the surveillance video with great efficiency.},
author = {Li, Ke and Yan, Bo and Wang, Weiyi and Gharavi, Hamid},
doi = {10.1109/LSP.2015.2496558},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Li et al.{\_}2016{\_}An Effective Video Synopsis Approach with Seam Carving.pdf:pdf},
isbn = {1070-9908 VO - 23},
issn = {1070-9908},
journal = {IEEE Signal Processing Letters},
keywords = {Electron tubes,Object tracking,Optical distortion,Optical imaging,Optical signal processing,Seam carving,Surveillance,object tracking,object tracking method,seam carving method,surveillance camera,surveillance video,video surveillance,video synopsis,video synopsis approach},
month = {jan},
number = {1},
pages = {11--14},
title = {{An Effective Video Synopsis Approach with Seam Carving}},
url = {http://ieeexplore.ieee.org/document/7312927/},
volume = {23},
year = {2016}
}
@article{Fu2014,
abstract = {With the explosive growth of surveillance video data, video synopsis technology is presented for fast browsing a day's worth of video in several minutes. However, for most existing solutions, motion structure in original videos may be destroyed even considering the temporal consistency of related objects. To maintain the important context cues, in this paper, we propose an online motion structure preserved synopsis approach, which can preserve behavior interactions between different objects in the original video while condensing as much content as possible. By measuring sociological proximity of moving objects, we introduce motion structure as a refined term directly added to the problem of energy minimization. A hierarchical fashion is employed to efficiently search an optimal solution for the problem of video synopsis, in which both the spatial collision and the temporal consistency are considered. Experimental results on extensive videos demonstrate the promise of the proposed approach. ?? 2014 Elsevier B.V.},
annote = {구현하기 위해서는 competative algorithm을 알아야되는데... 다음 입력으로 들어올 tube의 probability를 계산하고 그 probability에 따라서 선택하는 것 같음. 랜덤하게 결과가 변하지 않을까 싶은데?

성능 평가 measure는 별로인 것 같다. 비교 방법도 이상하고...},
author = {Fu, Wei and Wang, Jinqiao and Gui, Liangke and Lu, Hanqing and Ma, Songde},
doi = {10.1016/j.neucom.2013.12.041},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Fu et al.{\_}2014{\_}Online video synopsis of structured motion.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Motion structure,Video synopsis},
month = {jul},
pages = {155--162},
title = {{Online video synopsis of structured motion}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231214000666},
volume = {135},
year = {2014}
}
@article{ZhuangLi2009,
abstract = {Efficient browsing of long video sequences is a key tool in visual surveillance, e.g., for postevent video forensics, but can also be used for fast review of motion pictures and home videos. While frame skipping (fixed or adaptive) is straightforward to implement, its performance is quite limited. Although more efficient techniques have been developed, such as video summarization and video montage, they lose either the temporal or semantic context of events. A recently proposed method called video synopsis deals with some of these issues but involves multiple processing stages and is fairly complex. Video condensation, that we propose here, is novel in the way information is removed from the space-time video volume, is conceptually simple and relatively easy to implement. We introduce the concept of a video ribbon inspired by that of a seam recently proposed for image resizing. We recursively carve ribbons out by minimizing an activity-aware cost function using dynamic programming. The ribbon model we develop is flexible and permits an easy adjustment of the compromise between temporal condensation ratio and anachronism of events. We also propose sliding-window ribbon carving to handle streaming video and demonstrate the method's efficiency on motor and pedestrian traffic data.},
author = {{Zhuang Li} and Ishwar, Prakash and Konrad, Janusz},
doi = {10.1109/TIP.2009.2026677},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Zhuang Li, Ishwar, Konrad{\_}2009{\_}Video condensation by ribbon carving.pdf:pdf},
isbn = {1057-7149},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
keywords = {Nonuniform downsampling,Ribbon carving,Seam carving,Video summarization},
month = {nov},
number = {11},
pages = {2572--2583},
pmid = {19586819},
title = {{Video Condensation by Ribbon Carving}},
url = {http://ieeexplore.ieee.org/document/5156267/},
volume = {18},
year = {2009}
}
@article{Wang2013,
abstract = {The traditional pixel-domain based video analysis methods have taken dominated places for long. However, due to the rapidly increasing volume and resolution of surveillance video, the desirable fast and scalable browsing encounters significant challenges in terms of efficiency and flexibility. Under this circumstance, operating surveillance video in compressed domain has aroused great concern in academy and industry. In order to perform the intelligent video analysis task on the premise of preserving accuracy and controlling complexity, this paper presents a compressed-domain approach for massive surveillance video synopsis generation, labeling and browsing. The main work and achievements include: (1) a compressed-domain scheme is established to condense the compressed surveillance video and record the synopsis results; (2) a background modeling method via the Motion Vector based Local Binary Pattern (MVLBP) is introduced to extract moving objects in an efficient way; (3) an object flags based synopsis labeling method is proposed to represent the object regions as well as their display modes in a flexible way. Experimental results show that the video analysis system based on this framework can provide not only efficient synopsis generation but also flexible scalable or playback browsing. ?? 2013 Elsevier Inc. All rights reserved.},
author = {Wang, Shi-zheng and Wang, Zhong-yuan and Hu, Rui-min},
doi = {10.1016/j.jvcir.2013.10.001},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Wang, Wang, Hu{\_}2013{\_}Surveillance video synopsis in the compressed domain for fast video browsing.pdf:pdf},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Background modeling,Compressed domain,Fast browsing,Intelligent video,Scalable browsing,Surveillance video,Video labeling,Video synopsis},
month = {nov},
number = {8},
pages = {1431--1442},
title = {{Surveillance video synopsis in the compressed domain for fast video browsing}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1047320313001818},
volume = {24},
year = {2013}
}
@article{Lin2015a,
abstract = {In this paper, we propose a new approach to detect abnormal activities in surveillance videos and create suitable summary videos accordingly. The proposed approach first introduces a patch-based method to automatically model normal activity patterns and key regions in a scene. In this way, abnormal activities can be effectively detected and classified from the modeled normal patterns and key regions. Then, a blob sequence optimization process is proposed which integrates spatial, temporal, size, and motion correlation among objects to extract suitable foreground blob sequences for abnormal objects. With this process, blob extraction errors due to occlusion or background interference can be effectively avoided. Finally, we also propose an abnormality-type-based method which creates short-period summary videos from long-period input surveillance videos by properly arranging abnormal blob sequences according to their activity types. Experimental results show that our proposed approach can effectively create satisfying summary videos from input surveillance videos.},
annote = {Cd2un Times Cited:5 Cited References Count:37},
author = {Lin, Weiyao and Zhang, Yihao and Lu, Jiwen and Zhou, Bing and Wang, Jinjun and Zhou, Yu},
doi = {10.1016/j.neucom.2014.12.044},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Lin et al.{\_}2015{\_}Summarizing surveillance videos with local-patch-learning-based abnormality detection, blob sequence optimization, and t.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Abnormality detection,Blob sequence optimization,Video synopsis},
language = {English},
month = {may},
pages = {84--98},
title = {{Summarizing surveillance videos with local-patch-learning-based abnormality detection, blob sequence optimization, and type-based synopsis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231214017081},
volume = {155},
year = {2015}
}
@article{Li2016a,
abstract = {Video synopsis is an effective technique to provide a compact representation of the original video by removing spatiotemporal redundancies and by preserving the essential activities. Most current approaches for video synopsis will cause collisions among objects, especially when the video is condensed much. In this paper, we present an approach for video synopsis to reduce the collisions. Our approach first shifts active objects along the time axis to compact the original video. Then, the sizes of the objects are reduced when collisions occur. Meanwhile, the geometric centroids of the objects will be kept unchanged to preserve the location information. Our contributions are threefold. First, an approach is proposed to decrease collisions in the synopsis video through reducing the sizes of the objects. Second, an optimization framework is developed to indicate the optimal time position and the appropriate reduction coefficient for each object. Finally, some metrics are proposed, and several experiments are carried out to evaluate the proposed approach. The experiments have demonstrated that the synopsis video produced by our approach has much fewer collisions while the compression ratio is high.},
annote = {Dw8ky Times Cited:3 Cited References Count:53},
author = {Li, Xuelong and Wang, Zhigang and Lu, Xiaoqiang},
doi = {10.1109/TIP.2015.2507942},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Li, Wang, Lu{\_}2016{\_}Surveillance Video Synopsis via Scaling Down Objects.pdf:pdf},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
keywords = {Video synopsis,optimization,reduce collision,reduce size,surveillance},
language = {English},
month = {feb},
number = {2},
pages = {740--755},
title = {{Surveillance Video Synopsis via Scaling Down Objects}},
url = {http://ieeexplore.ieee.org/document/7353185/},
volume = {25},
year = {2016}
}
@inproceedings{Tran2011,
abstract = {We propose a novel algorithm for video event detection and localization as the optimal path discovery problem in spatio-temporal video space. By finding the optimal spatio-temporal path, our method not only detects the starting and ending points of the event, but also accurately locates it in each video frame. Moreover, our method is robust to the scale and intra-class variations of the event, as well as false and missed local detections, therefore improves the overall detection and localization accuracy. The proposed search algorithm obtains the global optimal solution with proven lowest computational complexity. Experiments on realistic video datasets demonstrate that our proposed method can be applied to different types of event detection tasks, such as abnormal event detection and walking pedestrian detection.},
author = {Tran, Du and Yuan, Junsong},
booktitle = {2011 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2011.5995416},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Tran, Yuan{\_}2011{\_}Optimal spatio-temporal path discovery for video event detection.pdf:pdf},
isbn = {978-1-4577-0394-2},
issn = {10636919},
month = {jun},
pages = {3321--3328},
publisher = {IEEE},
title = {{Optimal spatio-temporal path discovery for video event detection}},
url = {http://ieeexplore.ieee.org/document/5995416/},
year = {2011}
}
@inproceedings{Sun2012,
abstract = {By segmenting moving objects out and then densely stitching them into background frames, video synopsis provides an efficient way to condense long videos while preserving most activities. Existing video synopsis methods, however, often suffer from either high computation cost due to global energy minimization or unsatisfactory condense rate to avoid loss of important object activities. To address these problems, a tracking based fast online video synopsis approach is proposed in this paper which makes following three main contributions: 1) an online formulation of the video synopsis problem which makes the approach very fast and scalable to endless surveillance videos with reduced chronological disorders, 2) a tracking based schema which can preserve most object activities, and 3) a complete optimization process from both temporal and spatial redundancies of the video which results in much higher condense rate and less object conflict rate. Experimental results demonstrate the effectiveness and efficiency of proposed approach compared to the traditional method on public surveillance videos.},
author = {Sun, Lei and Xing, Junliang and Ai, Haizhou and Lao, Shihong},
booktitle = {Pattern Recognition (ICPR), 2012 21st International Conference on},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Sun et al.{\_}2012{\_}A tracking based fast online complete video synopsis approach.pdf:pdf},
isbn = {978-4-9906441-0-9},
issn = {1051-4651},
keywords = {image forensics,image motion analysis,image segmen},
pages = {1956--1959},
publisher = {IEEE},
title = {{A tracking based fast online complete video synopsis approach}},
year = {2012}
}
@article{Nguyen2016,
abstract = {Vision-based detection of illegal or accidental activities in urban traffic has attracted great interest. Since state-of-the-art online automated detection algorithms are far from perfect, much research effort on offline video surveillance has been made to prevent police or security staff from observing all recorded video frames unnecessarily. To solve the problem, this study focuses on video condensation, which provides fast monitoring of moving objects in a long duration of surveillance videos. Considering the computational complexity and the condensation ratio as the two main criteria for efficient video condensation, we propose a video condensation algorithm, which consists of the following: 1) initial condensation by discarding frames of nonmoving objects; 2) intra-GoFM (group of frames with moving objects) condensation; and 3) inter-GoFM condensation. In the intra-GoFM and inter-GoFM condensation, spatiotemporal static pixels within each GoFM and temporal static pixels between two consecutive GoFMs are dropped to shorten the temporal distances between consecutive moving objects. Experimental results show that our video condensation saves a significant amount of computational loads compared with the previous methods without sacrificing the condensation ratio and visual quality.},
annote = {Dv2tn Times Cited:0 Cited References Count:20},
author = {Nguyen, Hai Thanh and Jung, Seung-Won and Won, Chee Sun},
doi = {10.1109/TITS.2016.2518622},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Nguyen, Jung, Won{\_}2016{\_}Order-Preserving Condensation of Moving Objects in Surveillance Videos.pdf:pdf},
issn = {1524-9050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Video signal processing,ribbon carving,transportation surveillance video,video condensation},
language = {English},
month = {sep},
number = {9},
pages = {2408--2418},
title = {{Order-Preserving Condensation of Moving Objects in Surveillance Videos}},
url = {http://ieeexplore.ieee.org/document/7422070/},
volume = {17},
year = {2016}
}
@inproceedings{Pritch2007,
author = {Pritch, Yael and Rav-Acha, Alex and Gutman, Avital and Peleg, Shmuel},
booktitle = {Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Pritch et al.{\_}2007{\_}Webcam Synopsis Peeking Around the World.pdf.pdf:pdf},
isbn = {1424416302},
pages = {1--8},
publisher = {IEEE},
title = {{Webcam Synopsis Peeking Around the World.pdf}},
year = {2007}
}
@inproceedings{Lu2013,
abstract = {Video synopsis is one of the effective techniques to build a short video representation preserving the essential activities for a long video. Existing methods usually have the problem that a continuous activity (tube) from a single moving object is separated to a few small pieces. In this paper, two schemes are proposed to generate fluent tubes for video synopsis. The Gaussian mixture model and a texture method are combined to detect more compact foreground with shadow removed. The foreground constitutes a set of initial trajectories. A particle filter tracker is used to concatenate two trajectories if they belong to the same foreground activity, which generates more fluent tubes for video synopsis. Experimental results on 4 videos show that our method produces better accuracies and visual effects in video synopsis.},
author = {Lu, Minlong and Wang, Yueming and Pan, Gang},
booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
doi = {10.1109/ICASSP.2013.6638063},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Lu, Wang, Pan{\_}2013{\_}Generating fluent tubes in video synopsis.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {15206149},
keywords = {fluent tube,particle filter,shadow removal,video synopsis},
month = {may},
pages = {2292--2296},
publisher = {IEEE},
title = {{Generating fluent tubes in video synopsis}},
url = {http://ieeexplore.ieee.org/document/6638063/},
year = {2013}
}
@article{Pritch2008,
abstract = {The amount of captured video is growing with the increased numbers of video cameras, especially the increase of millions of surveillance cameras that operate 24 hours a day. Since video browsing and retrieval is time consuming, most captured video is never watched or examined. Video synopsis is an effective tool for browsing and indexing of such a video. It provides a short video representation, while preserving the essential activities of the original video. The activity in the video is condensed into a shorter period by simultaneously showing multiple activities, even when they originally occurred at different times. The synopsis video is also an index into the original video by pointing to the original time of each activity. Video Synopsis can be applied to create a synopsis of an endless video streams, as generated by webcams and by surveillance cameras. It can address queries like "Show in one minute the synopsis of this camera broadcast during the past day''. This process includes two major phases: (i) An online conversion of the endless video stream into a database of objects and activities (rather than frames). (ii) A response phase, generating the video synopsis as a response to the user's query.},
annote = {347ac Times Cited:99 Cited References Count:35},
author = {Pritch, Yael and Rav-Acha, Alex and Peleg, Shmuel},
doi = {10.1109/TPAMI.2008.29},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Pritch, Rav-Acha, Peleg{\_}2008{\_}Nonchronological Video Synopsis and Indexing.pdf:pdf},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Video indexing,Video summary,Video surveillance},
language = {English},
month = {nov},
number = {11},
pages = {1971--1984},
pmid = {18787245},
title = {{Nonchronological Video Synopsis and Indexing}},
url = {http://ieeexplore.ieee.org/document/4444355/},
volume = {30},
year = {2008}
}
@inproceedings{Wang2012,
abstract = {With extending applications of digital surveillance video, the fast browsing technique for surveillance video has become a hot spot in the domain. As one of the most important supporting technologies, region of interest (ROI) information coding is a necessary part of fast browsing framework for surveillance video. Therefore, a novel coding method of object region flags and successful application of object mapping flags for the scalable browsing of surveillance video are proposed in this paper. This object region flags coding method includes both intra-frame coding and inter-frame coding, wherein, the former eliminates the redundancy in spatial domain by improving the scanning mode of region flags, the latter eliminates the redundancy in temporal domain, which cuts coding cost obviously and provides essential supports for video synopsis browsing of surveillance video.},
author = {Wang, Shizheng and Liu, Heguang and Xie, Danfeng and Zeng, Binwei},
booktitle = {2012 Visual Communications and Image Processing},
doi = {10.1109/VCIP.2012.6410771},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Wang et al.{\_}2012{\_}A novel scheme to code object flags for video synopsis.pdf:pdf},
isbn = {978-1-4673-4407-4},
keywords = {Surveillance video,object flags,scalable browsing,video synopsis},
month = {nov},
pages = {1--5},
publisher = {IEEE},
title = {{A novel scheme to code object flags for video synopsis}},
url = {http://ieeexplore.ieee.org/document/6410771/},
year = {2012}
}
@article{Zhu2014,
abstract = {Nowadays, tremendous amount of video is cap- tured endlessly from increased numbers of video cameras distributed around the world. Since needless information is abundant in the raw videos, making video browsing and retrieval is inefficient and time consuming. Video synopsis is an effective way to browse and index such video, by pro- ducing a short video representation, while keeping the essen- tial activities of the original video. However, video synopsis for single camera is limited in its view scope, while under- standing and monitoring overall activity for large scenarios is valuable and demanding. To solve the above issues, we propose a novel video synopsis algorithm for partially over- lapping camera network. Our main contributions reside in three aspects: First, our algorithm can generate video syn- opsis for large scenarios, which can facilitate understand- ing overall activities. Second, for generating overall activity, we adopt a novel unsupervised graph matching algorithm to associate trajectories across cameras. Third, a novel multi- ple kernel similarity is adopted in selecting key observations for eliminating content redundancy in video synopsis. We have demonstrated the effectiveness of our approach on real surveillance videos captured by our camera network},
author = {Zhu, Xiaobin and Liu, Jing and Wang, Jinqiao and Lu, Hanqing},
doi = {10.1007/s00138-013-0519-8},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Zhu et al.{\_}2014{\_}Key observation selection-based effective video synopsis for camera network.pdf:pdf},
isbn = {9784990644116},
issn = {0932-8092},
journal = {Machine Vision and Applications},
keywords = {Camera network,Graph matching,Video surveillance,Video synopsis},
month = {jan},
number = {1},
pages = {145--157},
publisher = {IEEE},
title = {{Key observation selection-based effective video synopsis for camera network}},
url = {http://link.springer.com/10.1007/s00138-013-0519-8},
volume = {25},
year = {2014}
}
@inproceedings{Pritch2009,
abstract = {Millions of surveillance cameras record video around the clock, producing huge video archives. Even when a video archive is known to include critical activities, finding them is like finding a needle in a haystack, making the archive almost worthless. Two main approaches were proposed to address this problem: action recognition and video summarization. Methods for automatic detection of activities still face problems in many scenarios. The video synopsis approach to video summarization is very effective, but may produce confusing summaries by the simultaneous display of multiple activities.A new methodology for the generation of short and coherent video summaries is presented, based on clustering of similar activities. Objects with similar activities are easy to watch simultaneously, and outliers can be spotted instantly. Clustered synopsis is also suitable for efficient creation of ground truth data.},
author = {Pritch, Yael and Ratovitch, Sarit and Hendel, Avishai and Peleg, Shmuel},
booktitle = {Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance},
doi = {10.1109/AVSS.2009.53},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Pritch et al.{\_}2009{\_}Clustered Synopsis of Surveillance Video.pdf:pdf},
isbn = {978-1-4244-4755-8},
month = {Sep.},
pages = {195--200},
publisher = {IEEE Computer Society},
title = {{Clustered Synopsis of Surveillance Video}},
url = {http://ieeexplore.ieee.org/document/5280098/},
year = {2009}
}
@article{Zhu2016,
abstract = {Due to an increasing demand of video surveillance, there is an explosive growth of surveillance videos, which causes a big challenge in video storage, browsing and retrieval. The video synopsis technique is thus developed to extract and rearrange moving objects so as to handle the massive video browsing challenge. However, the traditional video synopsis (TVS) method only considers processing videos captured by a single camera, ignoring object interactions in multi-camera videos. To address this issue, we propose a novel multi-camera joint video synopsis (JVS) algorithm for multi-camera surveillance videos. Firstly, a key time stamp (KTS) selection method is designed to find an object's appearing, merging, splitting, and disappearing moments in the frame sequence, called tube, of that object. Secondly, tubes are rearranged by minimizing a global energy function that involves the overall camera views. Compared with the energy function used in TVS, the proposed global energy function considers the chronological orders of tubes not only in the same camera view, but also among different camera views. Moreover, the chronological disorder cost term is formulated based on the KTS labels, and improved by considering the visual similarity between two tubes. Finally, the multi-camera synopsis videos are separately generated by stitching together the globally rearranged tubes and background images of the same camera view. Extensive experiments show that the proposed JVS method is better than the traditional single-camera video synopsis method in preserving chronological orders of moving objects among multi-camera synopsis videos},
author = {Zhu, Jianqing and Liao, Shengcai and Li, Stan Z.},
doi = {10.1109/TCSVT.2015.2430692},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Zhu, Liao, Li{\_}2016{\_}Multicamera Joint Video Synopsis.pdf:pdf},
issn = {1051-8215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Camera network,joint video synopsis (JVS),video surveillance},
month = {jun},
number = {6},
pages = {1058--1069},
title = {{Multicamera Joint Video Synopsis}},
url = {http://ieeexplore.ieee.org/document/7103038/},
volume = {26},
year = {2016}
}
@inproceedings{ShikunFeng2012,
abstract = {Explosive growth of surveillance video data presents formidable challenges to its browsing, retrieval and stor- age. Video synopsis, an innovation proposed by Peleg and his colleagues, is aimed for fast browsing by shortening the video into a synopsis while keeping activities in video cap- tured by a camera. However, the current techniques are offline methods requiring that all the video data be ready for the processing, and are expensive in time and space. In this paper, we propose an online and efficient solution, and its supporting algorithms to overcome the problems. The method adopts an online content-aware approach in a step- wise manner, hence applicable to endless video, with less computational cost. Moreover, we propose a novel track- ing method, called sticky tracking, to achieve high-quality visualization. The system can achieve a faster-than-real- time speed with a multi-core CPU implementation. The ad- vantages are demonstrated by extensive experiments with a wide variety of videos. The proposed solution and algo- rithms could be integrated with surveillance cameras, and impact the way that surveillance videos are recorded.},
author = {{Shikun Feng} and {Zhen Lei} and {Dong Yi} and Li, Stan Z.},
booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6247913},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Shikun Feng et al.{\_}2012{\_}Online content-aware video condensation.pdf:pdf},
isbn = {978-1-4673-1228-8},
issn = {10636919},
month = {jun},
pages = {2082--2087},
publisher = {IEEE},
title = {{Online content-aware video condensation}},
url = {http://ieeexplore.ieee.org/document/6247913/},
year = {2012}
}
@article{Nie2014,
abstract = {Video synopsis aims at removing video's less important information, while preserving its key content for fast browsing, retrieving, or efficient storing. Previous video synopsis methods, including frame-based and object-based approaches that remove valueless whole frames or combine objects from time shots, cannot handle videos with redundancies existing in the movements of video object. In this paper, we present a novel part-based object movements synopsis method, which can effectively compress the redundant information of a moving video object and represent the synopsized object seamlessly. Our method works by part-based assembling and stitching. The object movement sequence is first divided into several part movement sequences. Then, we optimally assemble moving parts from different part sequences together to produce an initial synopsis result. The optimal assembling is formulated as a part movement assignment problem on a Markov Random Field (MRF), which guarantees the most important moving parts are selected while preserving both the spatial compatibility between assembled parts and the chronological order of parts. Finally, we present a non-linear spatiotemporal optimization formulation to stitch the assembled parts seamlessly, and achieve the final compact video object synopsis. The experiments on a variety of input video objects have demonstrated the effectiveness of the presented synopsis method.},
annote = {Ao7xr Times Cited:3 Cited References Count:42},
author = {Nie, Yongwei and Sun, Hanqiu and Li, Ping and Xiao, Chunxia and Ma, Kwan-Liu},
doi = {10.1109/TVCG.2013.2297931},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Nie et al.{\_}2014{\_}Object Movements Synopsis via Part Assembling and Stitching.pdf:pdf},
isbn = {1077-2626},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {MRF optimization,Video synopsis,belief propagation,part assembling,part stitching},
language = {English},
month = {sep},
number = {9},
pages = {1303--1315},
pmid = {26357379},
title = {{Object Movements Synopsis via Part Assembling and Stitching}},
url = {http://ieeexplore.ieee.org/document/6702519/},
volume = {20},
year = {2014}
}
@inproceedings{Rav-Acha2006,
abstract = {The power of video over still images is the ability to represent dynamic activities. But video browsing and retrieval are inconvenient due to inherent spatio-temporal redundancies, where some time intervals may have no activity, or have activities that occur in a small image region. Video synopsis aims to provide a compact video representation, while preserving the essential activities of the original video. We present dynamic video synopsis, where most of the activity in the video is condensed by simultaneously showing several actions, even when they originally occurred at different times. For example, we can create a "stroboscopic movie", where multiple dynamic instances of a moving object are played simultaneously. This is an extension of the still stroboscopic picture. Previous approaches for video abstraction addressed mostly the temporal redundancy by selecting representative key-frames or time intervals. In dynamic video synopsis the activity is shifted into a significantly shorter period, in which the activity is much denser. Video examples can be found online in http://www.vision.huji.ac.il/synopsis},
author = {Rav-Acha, Alex and Pritch, Yael and Peleg, Shmuel},
booktitle = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 1 (CVPR'06)},
doi = {10.1109/CVPR.2006.179},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Rav-Acha, Pritch, Peleg{\_}2006{\_}Making a Long Video Short Dynamic Video Synopsis.pdf:pdf},
isbn = {0-7695-2597-0},
issn = {10636919},
pages = {435--441},
pmid = {24256602},
publisher = {IEEE},
title = {{Making a Long Video Short: Dynamic Video Synopsis}},
url = {http://ieeexplore.ieee.org/document/1640790/},
volume = {1},
year = {2006}
}
@inproceedings{Chien-LiChou2015,
author = {{Chien-Li Chou} and {Chin-Hsien Lin} and {Tzu-Hsuan Chiang} and {Hua-Tsung Chen} and {Suh-Yin Lee}},
booktitle = {2015 IEEE International Conference on Multimedia {\&} Expo Workshops (ICMEW)},
doi = {10.1109/ICMEW.2015.7169855},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Chien-Li Chou et al.{\_}2015{\_}Coherent event-based surveillance video synopsis using trajectory clustering.pdf:pdf},
isbn = {978-1-4799-7079-7},
keywords = {Longest Common Subsequence (LCS),coherent event,surveillance,trajectory clustering,video summarization,video synopsis},
month = {jun},
pages = {1--6},
publisher = {IEEE},
title = {{Coherent event-based surveillance video synopsis using trajectory clustering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7169855},
year = {2015}
}
@article{Huang2014,
abstract = {To reduce human efforts in browsing long surveillance videos, synopsis videos are proposed. Traditional synopsis video generation applying optimization on video tubes is very time consuming and infeasible for real-time online generation. This dilemma significantly reduces the feasibility of synopsis video generation in practical situations. To solve this problem, the synopsis video generation problem is formulated as a maximum a posteriori probability (MAP) estimation problem in this paper, where the positions and appearing frames of video objects are chronologically rearranged in real time without the need to know their complete trajectories. Moreover, a synopsis table is employed with MAP estimation to decide the temporal locations of the incoming foreground objects in the synopsis video without needing an optimization procedure. As a result, the computational complexity of the proposed video synopsis generation method can be significantly reduced. Furthermore, as it does not require prescreening the entire video, this approach can be applied on online streaming videos.},
annote = {Foreground instance vs. foreground object

다른 online VS 방법과 구조가 달라서 현재 일반화 된 구현에 적용하기 어려울 것으로 예상.

Appearance model 만들 때 BGM에 사용되는 GMM과 같은 방식으로 만듬.

알고리즘 흐름 상 filtering 기능을 추가하기 어려울 것으로 판단됨.},
author = {Huang, Chun-Rong and Chung, Pau-Choo Julia and Yang, Di-Kai and Chen, Hsing-Cheng and Huang, Guan-Jie},
doi = {10.1109/TCSVT.2014.2308603},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Huang et al.{\_}2014{\_}Maximum italica Posterioriitalic Probability Estimation for Online Surveillance Video Synopsis.pdf:pdf},
issn = {1051-8215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Maximum a post er iori (MAP) estimation,video summarization,video surveillance,video synopsis},
language = {English},
month = {aug},
number = {8},
pages = {1417--1429},
title = {{Maximum {\textless}italic{\textgreater}a Posteriori{\textless}/italic{\textgreater} Probability Estimation for Online Surveillance Video Synopsis}},
url = {http://ieeexplore.ieee.org/document/6748870/},
volume = {24},
year = {2014}
}
@article{JianqingZhu2015,
abstract = {Video synopsis or condensation is a smart solution for fast video browsing and storage. However, most of the existing methods work offline, where two main phases are required. The first phase is to prepare tubes and background images. The second phase is to rearrange tubes and stitch them into backgrounds. However, with a long video sequence, the first phase is memory consuming for data storage, and the second phase is computationally expensive to rearrange all tubes simultaneously. To overcome these problems, we propose a high-performance video condensation system based on an online content-aware framework. The online framework transforms the optimization problem of tube rearrangement into a stepwise optimization problem. Therefore, it can condense video with much less memory and higher speed than the offline framework. With the aid of this transformation, the proposed system can process input videos and produce condensed videos simultaneously. Thus it is suitable for real-time endless surveillance videos. Meanwhile, the online mechanism allows users to directly visit the condensation video that has been generated. Moreover, the content-aware mechanism makes the proposed system able to automatically determine the duration of a condensed video. Finally, the proposed system uses Graphic Processing Unit (GPU) and multicore techniques to improve the speed. Extensive experiments that validate the high efficiency of the system are presented.},
annote = {여기서는 이전 논문(online content-aware video synopsis)에서 나왔던 roulette wheel selection 컨셉이 안들어가 있는 듯?

L1, L2 space의 의미와 temporary list의 의미를 헷갈리지 않도록 주의하자.},
author = {{Jianqing Zhu} and {Shikun Feng} and {Dong Yi} and {Shengcai Liao} and {Zhen Lei} and Li, Stan Z},
doi = {10.1109/TCSVT.2014.2363738},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Jianqing Zhu et al.{\_}2015{\_}High-Performance Video Condensation System.pdf:pdf},
issn = {1051-8215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {25,7,and systems for video,e transactions on circuits,july 2015,no,technology,vol},
language = {English},
month = {jul},
number = {7},
pages = {1113--1124},
title = {{High-Performance Video Condensation System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6928452 http://ieeexplore.ieee.org/document/6928452/},
volume = {25},
year = {2015}
}
@article{Cooharojananone2015,
abstract = {Reviewing video surveillance contents for security monitoring is a time-consuming and time-limiting task. This paper presents a real-time video surveillance summarization framework intended for minimizing the time requirement for time critical tasks, based on compact moving objects in time-space. A tunnel is proposed as an individual time-dimension object. In order to summarize an endless video into a shorter duration without loss of selected targets so as to extend the understanding of any given individual object, this research utilizes three real-time algorithms. Direct shift collision detection (DSCD) is implemented for the extremely fast shifting of tunnels together in time-space. The DSCD summarized video can then be customized by technique from many different approaches. Here, early trajectory searching is applied with the same DSCD technique, and then direct distance transform is used to instantly give the trajectory similarity between tunnels and the user's query. The most important step for identifying each individual object is background subtraction. To this end, dynamic region adaptation (DRA) was used as the background subtraction algorithm to select the best foreground for each object before making a tunnel. DRA also helps DSCD to summarize the video more accurately. The proposed framework is able to provide the results by real-time performance approach without losing the major events of the original video stream.},
annote = {Cp2zj Times Cited:0 Cited References Count:21},
author = {Cooharojananone, Nagul and Kasamwattanarote, Siriwat and Lipikorn, Rajalida and Satoh, Shin'ichi},
doi = {10.1007/s11554-012-0280-7},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Cooharojananone et al.{\_}2015{\_}Automated real-time video surveillance summarization framework.pdf:pdf},
issn = {1861-8200},
journal = {Journal of Real-Time Image Processing},
keywords = {Background subtraction,Direct shift collision detection,Distance transform,Dynamic region adaptation,Film map generation,Foreground extraction,HOG,Just-in-time renderer,Object tracking,Tunnel processing,Video summarization,adaptation {\'{a}} background,detection {\'{a}},distance transform {\'{a}} film,map generation {\'{a}} just-in-time,renderer {\'{a}} dynamic region,tracking {\'{a}},tunnel processing {\'{a}} hog,video summarization {\'{a}} object,{\'{a}} direct shift collision},
language = {English},
month = {sep},
number = {3},
pages = {513--532},
title = {{Automated real-time video surveillance summarization framework}},
url = {http://link.springer.com/10.1007/s11554-012-0280-7},
volume = {10},
year = {2015}
}
@inproceedings{Zivkovic2004,
abstract = {Background subtraction is a common computer vision task. We analyze the usual pixel-level approach. We develop an efficient adaptive algorithm using Gaussian mixture probability density. Recursive equations are used to constantly update the parameters and but also to simultaneously select the appropriate number of components for each pixel.},
author = {Zivkovic, Zoran},
booktitle = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},
doi = {10.1109/ICPR.2004.1333992},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Zivkovic{\_}2004{\_}Improved adaptive Gaussian mixture model for background subtraction.pdf:pdf},
isbn = {0-7695-2128-2},
issn = {1051-4651},
number = {2},
pages = {28--31 Vol.2},
publisher = {IEEE},
title = {{Improved adaptive Gaussian mixture model for background subtraction}},
url = {http://ieeexplore.ieee.org/document/1333992/},
volume = {2},
year = {2004}
}
@article{Zivkovic2006,
	title={Efficient adaptive density estimation per image pixel for the task of background subtraction},
	author={Zivkovic, Zoran and Van Der Heijden, Ferdinand},
	journal={Pattern recognition letters},
	volume={27},
	number={7},
	pages={773--780},
	year={2006},
	publisher={Elsevier}
}
@article{maddalena2008self,
	title={A self-organizing approach to background subtraction for visual surveillance applications},
	author={Maddalena, Lucia and Petrosino, Alfredo},
	journal={IEEE Transactions on Image Processing},
	volume={17},
	number={7},
	pages={1168--1177},
	year={2008},
	publisher={IEEE}
}
@inproceedings{maddalena2012sobs,
	title={The SOBS algorithm: What are the limits?},
	author={Maddalena, Lucia and Petrosino, Alfredo},
	booktitle={2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	pages={21--26},
	year={2012},
	organization={IEEE}
}
@article{cuevas2013improved,
	title={Improved background modeling for real-time spatio-temporal non-parametric moving object detection strategies},
	author={Cuevas, Carlos and Garc{\'\i}a, Narciso},
	journal={Image and Vision Computing},
	volume={31},
	number={9},
	pages={616--630},
	year={2013},
	publisher={Elsevier}
}
@article{haines2014background,
	title={Background subtraction with dirichletprocess mixture models},
	author={Haines, Tom SF and Xiang, Tao},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	volume={36},
	number={4},
	pages={670--683},
	year={2014},
	publisher={IEEE}
}
@article{cuevas2016labeled,
	title={Labeled dataset for integral evaluation of moving object detection algorithms: LASIESTA},
	author={Cuevas, Carlos and Y{\'a}{\~n}ez, Eva Mar{\'\i}a and Garc{\'\i}a, Narciso},
	journal={Computer Vision and Image Understanding},
	volume={152},
	pages={103--117},
	year={2016},
	publisher={Elsevier}
}
@article{berjon2018real,
	title={Real-time nonparametric background subtraction with tracking-based foreground update},
	author={Berj{\'o}n, Daniel and Cuevas, Carlos and Mor{\'a}n, Francisco and Garc{\'\i}a, Narciso},
	journal={Pattern Recognition},
	volume={74},
	pages={156--170},
	year={2018},
	publisher={Elsevier}
}
@inproceedings{Barnich2009ViBe,
	title = {{ViBe}: a powerful random technique to estimate the background in video sequences},
	author = {O. Barnich and M. {Van Droogenbroeck}},
	booktitle = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2009)},
	pages = {945-948},
	month = {April},
	year = {2009},
	note = {PDF available on the University site or at the IEEE},
	keywords = {ViBe, Video, Background subtraction, Background modelling, Motion detection},
	pdf = {http://orbi.ulg.ac.be/bitstream/2268/12087/1/Barnich2009ViBe.pdf},
	url = {http://hdl.handle.net/2268/12087} 
}

@article{Barnich2011ViBe,
	title = {{ViBe}: A universal background subtraction algorithm for video sequences},
	author = {O. Barnich and M. {Van Droogenbroeck}},
	journal = {IEEE Transactions on Image Processing},
	volume = {20},
	number = {6},
	pages = {1709-1724},
	month = {June},
	year = {2011},
	keywords = {ViBe, Background, Background subtraction, Segmentation, Motion, Motion detection},
	pdf = {http://orbi.ulg.ac.be/bitstream/2268/81248/1/Barnich2011ViBe.pdf},
	doi = {10.1109/TIP.2010.2101613},
	url = {http://hdl.handle.net/2268/81248} 
}

@inproceedings{VanDroogenbroeck2012Background,
	title = {Background Subtraction: Experiments and Improvements for {ViBe}},
	author = {M. {Van Droogenbroeck} and O. Paquot},
	booktitle = {Change Detection Workshop (CDW)},
	month = {June},
	year = {2012},
	address = {Providence, Rhode Island},
	keywords = {ViBe, Video, Background subtraction, Background modelling, Motion detection},
	pdf = {http://orbi.ulg.ac.be/bitstream/2268/117561/1/VanDroogenbroeck2012Background.pdf},
	url = {http://hdl.handle.net/2268/117561} 
}

@incollection{VanDroogenbroeck2014ViBe,
	title = {{ViBe}: A Disruptive Method for Background Subtraction},
	author = {M. {Van Droogenbroeck} and O. Barnich},
	booktitle = {Background Modeling and Foreground Detection for Video Surveillance},
	editor = {T. Bouwmans and F. Porikli and B. Hoferlin and A. Vacavant},
	chapter = {7},
	pages = {7.1-7.23},
	month = {July},
	year = {2014},
	publisher = {Chapman and Hall/CRC},
	doi = {10.1201/b17223-10},
	url = {http://hdl.handle.net/2268/157176},
	url = {http://www.telecom.ulg.ac.be/publi/publications/mvd/VanDroogenbroeck2014ViBe/},
	pdf = {http://orbi.ulg.ac.be/bitstream/2268/157176/1/VanDroogenbroeck2014ViBe.pdf},
	keywords = {Background subtraction, ViBe, Book} 
}
@inproceedings{Hofmann2012,
	abstract = {In this paper we present a novel method for foreground segmentation. Our proposed approach follows a non-parametric background modeling paradigm, thus the background is modeled by a history of recently observed pixel values. The foreground decision depends on a decision threshold. The background update is based on a learning parameter. We extend both of these parameters to dynamic per-pixel state variables and introduce dynamic controllers for each of them. Furthermore, both controllers are steered by an estimate of the background dynamics. In our experiments, the proposed Pixel-Based Adaptive Segmenter (PBAS) outperforms most state-of-the-art methods. View full abstract},
	author = {Hofmann, Martin and Tiefenbacher, Philipp and Rigoll, Gerhard},
	booktitle = {2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	doi = {10.1109/CVPRW.2012.6238925},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Hofmann, Tiefenbacher, Rigoll - 2012 - Background segmentation with feedback The Pixel-Based Adaptive Segmenter.pdf:pdf},
	isbn = {978-1-4673-1612-5},
	issn = {21607508},
	mendeley-groups = {background modeling},
	month = {jun},
	pages = {38--43},
	publisher = {IEEE},
	title = {{Background segmentation with feedback: The Pixel-Based Adaptive Segmenter}},
	url = {http://ieeexplore.ieee.org/document/6238925/},
	year = {2012}
}
@article{Muchtar2018,
	abstract = {Foreground segmentation is one of moving object detection techniques of computer vision applications. To date, modern moving object detection methods require complex background modeling and thresholds tuning to confront illumination changes. This paper proposes an adaptive approach based on non-overlapping block texture representation. It aims to design a computationally light and efficient solution to improve the robustness of detection. We evaluate our proposed method on internal and public sequences and provide the quantitative and qualitative measurements. Experimental results show that the proposed method can improve the results of previous method and suitable for real-time challenges.},
	author = {Muchtar, Kahlil and Rahman, Faris and Cenggoro, Tjeng Wawan and Budiarto, Arif and Pardamean, Bens},
	doi = {10.1016/j.procs.2018.08.228},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Muchtar et al. - 2018 - An Improved Version of Texture-based Foreground Segmentation Block-based Adaptive Segmenter.pdf:pdf},
	issn = {18770509},
	journal = {Procedia Computer Science},
	keywords = {change detection,foreground segmentation,texture features},
	mendeley-groups = {background modeling},
	pages = {579--586},
	publisher = {Elsevier B.V.},
	title = {{An Improved Version of Texture-based Foreground Segmentation: Block-based Adaptive Segmenter}},
	url = {https://doi.org/10.1016/j.procs.2018.08.228 https://linkinghub.elsevier.com/retrieve/pii/S1877050918315242},
	volume = {135},
	year = {2018}
}
@article{Kirkpatrick1983,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kirkpatrick, S and Gelatt, C D and Vecchi, M P},
doi = {10.1126/science.220.4598.671},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Kirkpatrick, Gelatt, Vecchi{\_}1983{\_}Optimization by Simulated Annealing.pdf:pdf},
isbn = {9788578110796},
issn = {0036-8075},
journal = {Science},
month = {may},
number = {4598},
pages = {671--680},
pmid = {25246403},
title = {{Optimization by Simulated Annealing}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.220.4598.671},
volume = {220},
year = {1983}
}
@article{Perez2003,
abstract = {Using generic interpolation machinery based on solving Poisson equations, a variety of novel tools are introduced for seamless editing of image regions. The rst set of tools permits the seamless importation of both opaque and transparent source image regions into a destination region. The second set is based on similar mathematical ideas and allows the user to modify the appearance of the image seamlessly, within a selected region. These changes can be arranged to affect the texture, the illumination, and the color of objects lying in the region, or to make tileable a rectangular selection.},
author = {P{\'{e}}rez, Patrick and Gangnet, Michel and Blake, Andrew},
doi = {10.1145/882262.882269},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Patrick, Gangnet, Blake{\_}2003{\_}Poisson Image Editing.pdf:pdf},
isbn = {1581137095},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {guided in-,image gradient,interactive image editing,poisson equation,seamless cloning,selection editing,terpolation},
number = {3},
pages = {313},
pmid = {16786694},
title = {{Poisson image editing}},
url = {http://portal.acm.org/citation.cfm?doid=882262.882269},
volume = {22},
year = {2003}
}
@article{Hoferlin2011,
abstract = {Automated video analysis lacks reliability when searching for unknown events in video data. The practical approach is to watch all the recorded video data, if applicable in fast-forward mode. In this paper we present a method to adapt the playback velocity of the video to the temporal information density, so that the users can explore the video under controlled cognitive load. The proposed approach can cope with static changes and is robust to video noise. First, we formulate temporal information as symmetrized R{\'{e}}nyi divergence, deriving this measure from signal coding theory. Further, we discuss the animated visualization of accelerated video sequences and propose a physiologically motivated blending approach to cope with arbitrary playback velocities. Finally, we compare the proposed method with the current approaches in this field by experiments and a qualitative user study, and show its advantages over motion-based measures.},
author = {H{\"{o}}ferlin, Benjamin and H{\"{o}}ferlin, Markus and Weiskopf, Daniel and Heidemann, Gunther},
doi = {10.1007/s11042-010-0606-z},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/H{\"{o}}ferlin et al.{\_}2011{\_}Information-based adaptive fast-forward for visual surveillance.pdf:pdf},
issn = {1380-7501},
journal = {Multimedia Tools and Applications},
keywords = {Adaptive fast-forward,Information theory,Video browsing,Video summarization,Visual surveillance},
month = {oct},
number = {1},
pages = {127--150},
title = {{Information-based adaptive fast-forward for visual surveillance}},
url = {http://link.springer.com/10.1007/s11042-010-0606-z},
volume = {55},
year = {2011}
}
@inproceedings{Smith1998,
abstract = {Digital video is rapidly becoming important for education, entertainment and a host of multimedia applications. With the size of the video collections growing to thousands of hours, technology is needed to effectively browse segments in a short time without losing the content of the video. We propose a method to extract the significant audio and video information and create a skim video which represents a very short synopsis of the original. The goal of this work is to show the utility of integrating language and image understanding techniques for video skimming by extraction of significant information, such as specific objects, audio keywords and relevant video structure. The resulting skim video is much shorter; where compaction is as high as 20:1, and yet retains the essential content of the original segment. We have conducted a user-study to test the content summarization and effectiveness of the skim as a browsing tool},
author = {Smith, M.A. and Kanade, T.},
booktitle = {Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.1997.609414},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Smith, Kanade{\_}1998{\_}Video skimming and characterization through the combination of image and language understanding techniques.pdf:pdf},
isbn = {0-8186-7822-4},
issn = {1063-6919},
pages = {775--781},
publisher = {IEEE Comput. Soc},
title = {{Video skimming and characterization through the combination of image and language understanding techniques}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=609414 http://ieeexplore.ieee.org/document/609414/},
year = {1998}
}
@article{Kuhn1955,
author = {Kuhn, H. W.},
doi = {10.1002/nav.3800020109},
issn = {00281441},
journal = {Naval Research Logistics Quarterly},
month = {mar},
number = {1-2},
pages = {83--97},
title = {{The Hungarian method for the assignment problem}},
url = {http://doi.wiley.com/10.1002/nav.3800020109},
volume = {2},
year = {1955}
}
@article{kuhn1956variants,
	title={Variants of the Hungarian method for assignment problems},
	author={Kuhn, Harold W},
	journal={Naval Research Logistics Quarterly},
	volume={3},
	number={4},
	pages={253--258},
	year={1956},
	publisher={Wiley Online Library}
}
@article{munkres1957algorithms,
	title={Algorithms for the assignment and transportation problems},
	author={Munkres, James},
	journal={Journal of the society for industrial and applied mathematics},
	volume={5},
	number={1},
	pages={32--38},
	year={1957},
	publisher={SIAM}
}
@article{bourgeois1971extension,
	title={An extension of the Munkres algorithm for the assignment problem to rectangular matrices},
	author={Bourgeois, Fran{\c{c}}ois and Lassalle, Jean-Claude},
	journal={Communications of the ACM},
	volume={14},
	number={12},
	pages={802--804},
	year={1971},
	publisher={ACM}
}
@book{Oppenheim2009,
author = {Oppenheim, Alan V and Schafer, Ronald W},
edition = {3rd},
publisher = {Pearson},
isbn = {0132067099},
pages = {742--774},
title = {Discrete Time Signal Processing},
year = {2010}
}
@book{Cormen2009,
author = {Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford},
edition = {3rd},
publisher = {MIT press},
isbn = {9780262033848},
pages = {414--450},
title = {Introduction to algorithms},
year = {2009}
}
@article{Petrovic2005,
abstract = {We derive a statistical graphical model of video scenes with multiple, possibly occluded objects that can be efficiently used for tasks related to video search, browsing and retrieval. The model is trained on query (target) clip selected by the user. Shot retrieval process is based on the likelihood of a video frame under generative model. Instead of using a combination of weighted Euclidean distances as a shot similarity measure, the likelihood model automatically separates and balances various causes of variability in video, including occlusion, appearance change and motion. Thus, we overcome tedious and complex user interventions required in previous studies. We use the model in the adaptive video forward application that adapts video playback speed to the likelihood of the data. The similarity measure of each candidate clip to the target clip defines the playback speed. Given a query, the video is played at a higher speed as long as video content has low likelihood, and when frames similar to the query clip start to come in, the video playback rate drops. Set of experiments o12n typical home videos demonstrate performance, easiness and utility of our application.},
author = {Petrovic, Nemanja and Jojic, Nebojsa and Huang, Thomas S.},
doi = {10.1007/s11042-005-0895-9},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Petrovic, Jojic, Huang{\_}2005{\_}Adaptive video fast forward.pdf:pdf},
issn = {1380-7501},
journal = {Multimedia Tools and Applications},
keywords = {Content-based retrieval,Generative models,Video fast forward},
mendeley-groups = {video synopsis/fast forward},
month = {aug},
number = {3},
pages = {327--344},
title = {{Adaptive Video Fast Forward}},
url = {http://link.springer.com/10.1007/s11042-005-0895-9},
volume = {26},
year = {2005}
}
@article{Kolmogorov2004,
abstract = {In the last few years, several new algorithms based on graph cuts have been developed to solve energy minimization problems in computer vision. Each of these techniques constructs a graph such that the minimum cut on the graph also minimizes the energy. Yet, because these graph constructions are complex and highly specific to a particular energy function, graph cuts have seen limited application to date. In this paper, we give a characterization of the energy functions that can be minimized by graph cuts. Our results are restricted to functions of binary variables. However, our work generalizes many previous constructions and is easily applicable to vision problems that involve large numbers of labels, such as stereo, motion, image restoration, and scene reconstruction. We give a precise characterization of what energy functions can be minimized using graph cuts, among the energy functions that can be written as a sum of terms containing three or fewer binary variables. We also provide a general-purpose construction to minimize such an energy function. Finally, we give a necessary condition for any energy function of binary variables to be minimized by graph cuts. Researchers who are considering the use of graph cuts to optimize a particular energy function can use our results to determine if this is possible and then follow our construction to create the appropriate graph. A software implementation is freely available.},
author = {Kolmogorov, Vladimir and Zabih, Ramin},
doi = {10.1109/TPAMI.2004.1262177},
file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Kolmogorov, Zabih{\_}2004{\_}What Energy Functions Can Be Minimized via Graph Cuts.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Energy minimization,Graph algorithms,Markov Random Fields,Maximum flow,Minimum cut,Optimization},
mendeley-groups = {video synopsis/references},
month = {feb},
number = {2},
pages = {147--159},
title = {{What energy functions can be minimized via graph cuts?}},
url = {http://ieeexplore.ieee.org/document/1262177/},
volume = {26},
year = {2004}
}
@article{Bouwmans2018,
	abstract = {Conventional neural networks show a powerful framework for background subtraction in video acquired by static cameras. Indeed, the well-known SOBS method and its variants based on neural networks were the leader methods on the largescale CDnet 2012 dataset during a long time. Recently, convolutional neural networks which belong to deep learning methods were employed with success for background initialization, foreground detection and deep learned features. Currently, the top current background subtraction methods in CDnet 2014 are based on deep neural networks with a large gap of performance in comparison on the conventional unsupervised approaches based on multi-features or multi-cues strategies. Furthermore, a huge amount of papers was published since 2016 when Braham and Van Droogenbroeck published their first work on CNN applied to background subtraction providing a regular gain of performance. In this context, we provide the first review of deep neural network concepts in background subtraction for novices and experts in order to analyze this success and to provide further directions. For this, we first surveyed the methods used background initialization, background subtraction and deep learned features. Then, we discuss the adequacy of deep neural networks for background subtraction. Finally, experimental results are presented on the CDnet 2014 dataset.},
	archivePrefix = {arXiv},
	arxivId = {1811.05255},
	author = {Bouwmans, Thierry and Javed, Sajid and Sultana, Maryam and Jung, Soon Ki},
	eprint = {1811.05255},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Bouwmans et al. - 2018 - Deep Neural Network Concepts for Background Subtraction A Systematic Review and Comparative Evaluation.pdf:pdf},
	mendeley-groups = {background modeling},
	month = {nov},
	title = {{Deep Neural Network Concepts for Background Subtraction: A Systematic Review and Comparative Evaluation}},
	url = {http://arxiv.org/abs/1811.05255},
	year = {2018}
}
@article{Sakkos2019,
	author = {Sakkos, Dimitrios and Ho, Edmond S. L. and Shum, Hubert P. H.},
	doi = {10.1109/ACCESS.2019.2891943},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Sakkos, Ho, Shum - 2019 - Illumination-Aware Multi-Task GANs for Foreground Segmentation.pdf:pdf},
	issn = {2169-3536},
	journal = {IEEE Access},
	keywords = {Background subtraction,generative adversarial networks,illumination-aware,multi-task learning,video segmentation},
	mendeley-groups = {background modeling},
	pages = {10976--10986},
	publisher = {IEEE},
	title = {{Illumination-Aware Multi-Task GANs for Foreground Segmentation}},
	url = {https://ieeexplore.ieee.org/document/8606933/},
	volume = {7},
	year = {2019}
}
@inproceedings{Bakkay2018,
	author = {Bakkay, M. C. and Rashwan, H. A. and Salmane, H. and Khoudour, L. and Puigtt, D. and Ruichek, Y.},
	booktitle = {2018 25th IEEE International Conference on Image Processing (ICIP)},
	doi = {10.1109/ICIP.2018.8451603},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Bakkay et al. - 2018 - BSCGAN Deep Background Subtraction with Conditional Generative Adversarial Networks.pdf:pdf},
	isbn = {978-1-4799-7061-2},
	issn = {15224880},
	keywords = {Background subtraction,Change detection,Deep learning,Generative Adversarial Networks},
	mendeley-groups = {background modeling},
	month = {oct},
	pages = {4018--4022},
	publisher = {IEEE},
	title = {{BSCGAN: Deep Background Subtraction with Conditional Generative Adversarial Networks}},
	url = {https://ieeexplore.ieee.org/document/8451603/},
	year = {2018}
}
@article{Sultana2019,
	abstract = {In many advanced video based applications background modeling is a pre-processing step to eliminate redundant data, for instance in tracking or video surveillance applications. Over the past years background subtraction is usually based on low level or hand-crafted features such as raw color components, gradients, or local binary patterns. The background subtraction algorithms performance suffer in the presence of various challenges such as dynamic backgrounds, photometric variations, camera jitters, and shadows. To handle these challenges for the purpose of accurate background modeling we propose a unified framework based on the algorithm of image inpainting. It is an unsupervised visual feature learning hybrid Generative Adversarial algorithm based on context prediction. We have also presented the solution of random region inpainting by the fusion of center region inpaiting and random region inpainting with the help of poisson blending technique. Furthermore we also evaluated foreground object detection with the fusion of our proposed method and morphological operations. The comparison of our proposed method with 12 state-of-the-art methods shows its stability in the application of background estimation and foreground detection.},
	author = {Sultana, Maryam and Mahmood, Arif and Javed, Sajid and Jung, Soon Ki},
	doi = {10.1007/s00138-018-0993-0},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Sultana et al. - 2019 - Unsupervised deep context prediction for background estimation and foreground segmentation.pdf:pdf},
	issn = {0932-8092},
	journal = {Machine Vision and Applications},
	keywords = {Background subtraction,Context prediction,Foreground detection,Generative adversarial networks},
	mendeley-groups = {background modeling},
	month = {apr},
	number = {3},
	pages = {375--395},
	publisher = {Springer Berlin Heidelberg},
	title = {{Unsupervised deep context prediction for background estimation and foreground segmentation}},
	url = {https://doi.org/10.1007/s00138-018-0993-0 http://link.springer.com/10.1007/s00138-018-0993-0},
	volume = {30},
	year = {2019}
}
@article{Patil2018,
	abstract = {IEEE Moving object detection (MOD) in videos is a challenging task. Estimation of accurate background is the key to extracting the foreground from video frames. In this paper, we have proposed a novel compact end-to-end convolutional neural network architecture, motion saliency foreground network (MSFgNet), to estimate the background and to extract the foreground from video frames. Initially, the long streaming video is divided into a number of small video streams (SVS). The proposed network takes the SVS as an input and estimates the background frame for each SVS. Second, the saliency map is extracted using the current video frame and estimated background. Furthermore, a compact encoder-decoder network is proposed to extract the foreground from the estimated saliency maps. The performance of the proposed MSFgNet is tested on three benchmark datasets (CDnet-2014, LASIESTA, and PTIS) for MOD. The computational complexity (handling of number of parameters and execution time) and the performance of the proposed MSFgNet are compared with the existing state-of-the-art methods for MOD in terms of precision, recall, and F-measure. Performance analysis shows that the proposed network is very compact and outperforms the existing state-of-the-art methods for MOD in videos.},
	author = {Patil, Prashant W. and Murala, Subrahmanyam},
	doi = {10.1109/TITS.2018.2880096},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Patil, Murala - 2018 - MSFgNet A Novel Compact End-to-End Deep Network for Moving Object Detection.pdf:pdf},
	issn = {1524-9050},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	keywords = {Background estimation,CNN.,Estimation,Feature extraction,Image color analysis,Lighting,Object detection,Saliency detection,Videos,deep learning,foreground,saliency},
	mendeley-groups = {background modeling},
	pages = {1--12},
	publisher = {IEEE},
	title = {{MSFgNet: A Novel Compact End-to-End Deep Network for Moving Object Detection}},
	url = {https://ieeexplore.ieee.org/document/8546771/},
	volume = {PP},
	year = {2018}
}
@article{Lim2018,
	abstract = {Several methods have been proposed to solve moving objects segmentation problem accurately in different scenes. However, many of them lack the ability of handling various difficult scenarios such as illumination changes, background or camera motion, camouflage effect, shadow etc. To address these issues, we propose two robust encoder-decoder type neural networks that generate multi-scale feature encodings in different ways and can be trained end-to-end using only a few training samples. Using the same encoder-decoder configurations, in the first model, a triplet of encoders take the inputs in three scales to embed an image in a multi-scale feature space; in the second model, a Feature Pooling Module (FPM) is plugged on top of a single input encoder to extract multi-scale features in the middle layers. Both models use a transposed convolutional network in the decoder part to learn a mapping from feature space to image space. In order to evaluate our models, we entered the Change Detection 2014 Challenge (changedetection.net) and our models, namely FgSegNet{\_}M and FgSegNet{\_}S, outperformed all the existing state-of-the-art methods by an average F-Measure of 0.9770 and 0.9804, respectively. We also evaluate our models on SBI2015 and UCSD Background Subtraction datasets. Our source code is made publicly available at https://github.com/lim-anggun/FgSegNet.},
	author = {Lim, Long Ang and {Yalim Keles}, Hacer},
	doi = {10.1016/j.patrec.2018.08.002},
	file = {:C$\backslash$:/Users/user/Documents/Mendeley Desktop/Lim, Yalim Keles{\_}2018{\_}Foreground segmentation using convolutional neural networks for multiscale feature encoding.pdf:pdf},
	issn = {01678655},
	journal = {Pattern Recognition Letters},
	keywords = {Background subtraction,Convolutional neural networks,Deep learning,Foreground segmentation,Pixel classification,Video surveillance},
	mendeley-groups = {background modeling},
	pages = {256--262},
	publisher = {Elsevier},
	title = {{Foreground segmentation using convolutional neural networks for multiscale feature encoding}},
	url = {https://doi.org/10.1016/j.patrec.2018.08.002 https://linkinghub.elsevier.com/retrieve/pii/S0167865518303702},
	volume = {112},
	year = {2018}
}

@article{Everingham15, 
	author = "Everingham, M. and Eslami, S. M. A. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.", 
	title = "The Pascal Visual Object Classes Challenge: A Retrospective", 
	journal = "International Journal of Computer Vision", 
	volume = "111", 
	year = "2015", 
	number = "1", 
	month = Jan, 
	pages = "98--136", 
} 
@inproceedings{Cordts2016Cityscapes,
	title={The Cityscapes Dataset for Semantic Urban Scene Understanding},
	author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
	booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
	publisher={IEEE}
	year={2016}
}
@article{Chen2014,
	abstract = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called "semantic image segmentation"). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our "DeepLab" system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6{\%} IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the 'hole' algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
	annote = {DeepLab-v1},
	archivePrefix = {arXiv},
	arxivId = {1412.7062},
	author = {Chen, Liang-chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
	eprint = {1412.7062},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2014 - Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs.pdf:pdf},
	mendeley-groups = {image segmentation},
	month = {Dec},
	pages = {1--14},
	title = {{Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs}},
	url = {http://arxiv.org/abs/1412.7062},
	year = {2014}
}
@inproceedings{Long2015,
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
	annote = {Reference implementation (Caffe)
	https://github.com/shelhamer/fcn.berkeleyvision.org
	
	Tensorflow implementation
	https://github.com/shekkizh/FCN.tensorflow},
	archivePrefix = {arXiv},
	arxivId = {1411.4038},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	doi = {10.1109/CVPR.2015.7298965},
	eprint = {1411.4038},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Long, Shelhamer, Darrell - 2015 - Fully convolutional networks for semantic segmentation.pdf:pdf},
	isbn = {978-1-4673-6964-0},
	mendeley-groups = {image segmentation},
	month = {Jun},
	pages = {3431--3440},
	publisher = {IEEE},
	title = {{Fully convolutional networks for semantic segmentation}},
	url = {http://arxiv.org/abs/1411.4038 http://ieeexplore.ieee.org/document/7298965/},
	year = {2015}
}
@article{Chen2017,
	abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
	archivePrefix = {arXiv},
	arxivId = {1706.05587},
	author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	eprint = {1706.05587},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image Segmentation.pdf:pdf},
	mendeley-groups = {image segmentation},
	month = {Jun},
	title = {{Rethinking Atrous Convolution for Semantic Image Segmentation}},
	url = {http://arxiv.org/abs/1706.05587},
	year = {2017}
}
@inproceedings{Zhao2017,
	abstract = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields new record of mIoU accuracy 85.4{\%} on PASCAL VOC 2012 and accuracy 80.2{\%} on Cityscapes.},
	archivePrefix = {arXiv},
	arxivId = {1612.01105},
	author = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	doi = {10.1109/CVPR.2017.660},
	eprint = {1612.01105},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Zhao et al. - 2017 - Pyramid Scene Parsing Network.pdf:pdf},
	isbn = {978-1-5386-0457-1},
	issn = {0162-8828},
	mendeley-groups = {image segmentation},
	month = {Jul},
	pages = {6230--6239},
	pmid = {28463186},
	publisher = {IEEE},
	title = {{Pyramid Scene Parsing Network}},
	url = {http://ieeexplore.ieee.org/document/8100143/},
	year = {2017}
}
@article{Chen2016,
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed "DeepLab" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7{\%} mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	annote = {DeepLab-v2},
	archivePrefix = {arXiv},
	arxivId = {1606.00915},
	author = {Chen, Liang-chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
	eprint = {1606.00915},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2016 - DeepLab Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.pdf:pdf},
	mendeley-groups = {image segmentation},
	month = {Jun},
	pages = {1--14},
	title = {{DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}},
	url = {http://arxiv.org/abs/1606.00915},
	year = {2016}
}
@article{Chen2018,
	abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0$\backslash${\%} and 82.1$\backslash${\%} without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at $\backslash$url{\{}https://github.com/tensorflow/models/tree/master/research/deeplab{\}}.},
	archivePrefix = {arXiv},
	arxivId = {1802.02611},
	author = {Chen, Liang-chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	eprint = {1802.02611},
	file = {:Users/user/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation.pdf:pdf},
	keywords = {and depthwise separable convolution,decoder,encoder-,semantic image segmentation,spatial pyramid pooling},
	mendeley-groups = {image segmentation},
	month = {Feb},
	title = {{Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation}},
	url = {http://arxiv.org/abs/1802.02611},
	year = {2018}
}
@inproceedings{perez2002color,
	title={Color-based probabilistic tracking},
	author={P{\'e}rez, Patrick and Hue, Carine and Vermaak, Jaco and Gangnet, Michel},
	booktitle={European Conference on Computer Vision},
	pages={661--675},
	year={2002},
	organization={Springer}
}